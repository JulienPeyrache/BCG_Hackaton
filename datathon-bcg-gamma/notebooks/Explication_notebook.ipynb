{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPORT ##\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explication Notebook : \n",
    "\n",
    "<p>Ce notebook est une explication de notre raisonnement pour obtenir nos résultats, certain code ne tourne plus correctement et d'autre sont très long à faire tourner. Le résultat final de notre expérience est dans le pipeline Kedro</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/11/22 18:03:25] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> No path argument was provided. Using: C:\\Users\\Erwan                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py:56</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BOULLIER\\Desktop\\Datathon\\BCG_Hackaton\\datathon-bcg-gamma               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/11/22 18:03:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m No path argument was provided. Using: C:\\Users\\Erwan                    \u001b[2m__init__.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m56\u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         BOULLIER\\Desktop\\Datathon\\BCG_Hackaton\\datathon-bcg-gamma               \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/11/22 18:03:25] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Kedro project Datathon BCG Gamma                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py:77</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/11/22 18:03:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Kedro project Datathon BCG Gamma                                        \u001b[2m__init__.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m77\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Defined global variable <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'session'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'catalog'</span> and <span style=\"color: #008000; text-decoration-color: #008000\">'pipelines'</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py:78</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Defined global variable \u001b[32m'context'\u001b[0m, \u001b[32m'session'\u001b[0m, \u001b[32m'catalog'\u001b[0m and \u001b[32m'pipelines'\u001b[0m \u001b[2m__init__.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m78\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Registered line magic <span style=\"color: #008000; text-decoration-color: #008000\">'run_viz'</span>                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py:84</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Registered line magic \u001b[32m'run_viz'\u001b[0m                                         \u001b[2m__init__.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m84\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading data from <span style=\"color: #008000; text-decoration-color: #008000\">'df_champs_elysee_days_meteo_bank'</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_catalog.py:343</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">(</span>ParquetDataSet<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading data from \u001b[32m'df_champs_elysee_days_meteo_bank'\u001b[0m               \u001b[2mdata_catalog.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m343\u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0mParquetDataSet\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m                                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading data from <span style=\"color: #008000; text-decoration-color: #008000\">'df_convention_days_meteo_bank'</span>                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_catalog.py:343</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">(</span>ParquetDataSet<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading data from \u001b[32m'df_convention_days_meteo_bank'\u001b[0m                  \u001b[2mdata_catalog.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m343\u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0mParquetDataSet\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m                                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/11/22 18:03:26] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading data from <span style=\"color: #008000; text-decoration-color: #008000\">'df_peres_days_meteo_bank'</span> <span style=\"font-weight: bold\">(</span>ParquetDataSet<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_catalog.py:343</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/11/22 18:03:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading data from \u001b[32m'df_peres_days_meteo_bank'\u001b[0m \u001b[1m(\u001b[0mParquetDataSet\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m   \u001b[2mdata_catalog.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m343\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_kedro\n",
    "df_champs = catalog.load('df_champs_elysee_days_meteo_bank')\n",
    "df_convention = catalog.load('df_convention_days_meteo_bank')\n",
    "df_saint_peres = catalog.load('df_peres_days_meteo_bank')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing :\n",
    "<p>Cette partie est consacrée au traitement des données. Nous effectuons deux traitements, le premier est le fênetrage (i.e l'ajout des données passées à l'input) et ensuite nous utilisons un pipeline Sklearn de normalisation des imputs</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fenetrage simple\n",
    "def series_to_supervised(df, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df[[\"Taux d'occupation\",'Débit horaire']].shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(2)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df[[\"Taux d'occupation\",'Débit horaire']].shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(2)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(2)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\tdf = df.merge(agg, left_index=True, right_index=True)\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tdf.dropna(inplace=True)\n",
    "\treturn df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fenetrage en utlisant la différence entre la valeur à T et T-1\n",
    "def series_to_supervised_dif_2(df, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df[[\"Taux d'occupation\",'Débit horaire']].shift(i) - df[[\"Taux d'occupation\",'Débit horaire']].shift(i+1))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(2)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    cols.append(df[[\"Taux d'occupation\",'Débit horaire']])\n",
    "    names += ['var_r1(t-1)', 'var_r2(t-1)']\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df[[\"Taux d'occupation\",'Débit horaire']].shift(-i) - df[[\"Taux d'occupation\",'Débit horaire']].shift(-1-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(2)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(2)]\n",
    "\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    df = df.merge(agg, left_index=True, right_index=True)\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline de normalization\n",
    "\n",
    "def create_pipeline(num_col, min_max_col):\n",
    "    ##Pipelines Scikit Learn\n",
    "    ct_scaler = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"scaler\", StandardScaler(), num_col),\n",
    "            (\"scaler_minmax\", MinMaxScaler(), min_max_col),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline_master = Pipeline([\n",
    "        (\"preprocessor\", ct_scaler),\n",
    "        ])\n",
    "    \n",
    "    return pipeline_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalisaton des outputs \n",
    "def normalize(data):\n",
    "    return (data - data.mean())/data.std()\n",
    "\n",
    "def unormalize(data, previous):\n",
    "    return (data*previous.std()) + previous.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalisation des outputs pour plusieurs predictions\n",
    "def normalize_n(data):\n",
    "    return (data - data.mean().mean())/data.std().mean()\n",
    "\n",
    "def unormalize_n(data, previous):\n",
    "    return (data*previous.std().mean()) + previous.mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction à T sans données Hexogènes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_only = catalog.load('champs_elysee_with_days_na_filled')\n",
    "df_time_only = df_time_only.drop(columns=['Libelle','Date et heure de comptage','date','vacances'])\n",
    "df_time_only = pd.get_dummies(df_time_only, columns=['year'], prefix='year')\n",
    "df_time_only = pd.get_dummies(df_time_only, columns=['hour'], prefix='hour')\n",
    "df_time_only = pd.get_dummies(df_time_only, columns=['day'], prefix='day')\n",
    "df_time_only = pd.get_dummies(df_time_only, columns=['day_of_week'], prefix='wday')\n",
    "df_time_only = pd.get_dummies(df_time_only, columns=['month'], prefix='month')\n",
    "\n",
    "\n",
    "df_time_only_preprocessd = series_to_supervised(df_time_only, 48, 1) #Using 48 past values\n",
    "df_time_only_preprocessd = df_time_only_preprocessd.drop(columns=[\"Taux d'occupation\",\"Débit horaire\"]) \n",
    "y_taux_t_time_only = df_time_only_preprocessd['var1(t)']\n",
    "y_debit_t_time_only = df_time_only_preprocessd['var2(t)']\n",
    "X_time_only = df_time_only_preprocessd.drop(columns=['var1(t)','var2(t)'])\n",
    "\n",
    "##Split\n",
    "offset = int(X_time_only.shape[0] * 0.9)\n",
    "X_train_time_only, y_taux_train_time_only, y_debit_train_time_only = X_time_only[:offset], y_taux_t_time_only[:offset], y_debit_t_time_only[:offset]\n",
    "X_test_time_only, y_taux_test_time_only, y_debit_test_time_only = X_time_only[offset:], y_taux_t_time_only[offset:], y_debit_t_time_only[offset:]\n",
    "\n",
    "\n",
    "#Models\n",
    "model_taux_time_only = xgb.XGBRegressor()\n",
    "model_taux_time_only = model_taux_time_only.fit(X_train_time_only,y_taux_train_time_only) \n",
    "\n",
    "model_debit_time_only = xgb.XGBRegressor()\n",
    "model_debit_time_only = model_debit_time_only.fit(X_train_time_only,y_debit_train_time_only) \n",
    "\n",
    "\n",
    "#Testing\n",
    "y_pred_taux_time_only_t = model_taux_time_only.predict(X_test_time_only)\n",
    "RMSE_taux_time_only = mean_squared_error(y_taux_test_time_only, y_pred_taux_time_only_t)\n",
    "\n",
    "\n",
    "y_pred_debit_time_only_t = model_debit_time_only.predict(X_test_time_only)\n",
    "RMSE_debit_time_only = mean_squared_error(y_debit_test_time_only, y_pred_debit_time_only_t)\n",
    "\n",
    "print(RMSE_taux_time_only**0.5,RMSE_debit_time_only**0.5)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(18, 5))\n",
    "n_taux_time_only = len(y_taux_test_time_only)\n",
    "t_taux_time_only = np.arange(n_taux_time_only)\n",
    "# plt.figure(\"Taux d'occupation\")\n",
    "ax[0,0].plot(t_taux_time_only,y_taux_test_time_only, 'r')\n",
    "ax[0,0].plot(t_taux_time_only,y_pred_taux_time_only_t, 'b')\n",
    "ax[0,0].title.set_text(\"Taux d'occupation\")\n",
    "\n",
    "n_debit_time_only= len(y_pred_debit_time_only_t)\n",
    "t_debit_time_only = np.arange(n_debit_time_only)\n",
    "# plt.figure(\"Débit horaire\")\n",
    "ax[0,1].plot(t_debit_time_only,y_debit_test_time_only, 'r')\n",
    "ax[0,1].plot(t_debit_time_only,y_pred_debit_time_only_t, 'b')\n",
    "ax[0,1].title.set_text(\"Débit horaire\")\n",
    "\n",
    "t_zoom = np.arange(100)\n",
    "ax[1,0].plot(t_zoom,y_taux_test_time_only[:100], 'r')\n",
    "ax[1,0].plot(t_zoom,y_pred_taux_time_only_t[:100], 'b')\n",
    "\n",
    "ax[1,1].plot(t_zoom,y_debit_test_time_only[:100], 'r')\n",
    "ax[1,1].plot(t_zoom,y_pred_debit_time_only_t[:100], 'b')\n",
    "fig.legend(['Real Data','Predicted'])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction à t en utilisant des données Hexogènes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_champs = series_to_supervised(df_champs, 48, 1) ## Using 48 past data\n",
    "df_preprocessed_champs = df_preprocessed_champs.drop(columns=[\"Taux d'occupation\",\"Débit horaire\"]) \n",
    "y_taux_t = df_preprocessed_champs['var1(t)']\n",
    "\n",
    "y_debit_t = df_preprocessed_champs['var2(t)']\n",
    "\n",
    "names_taux_drop = [('var2(t-%d)' % (i)) for i in range(1,48)]\n",
    "names_debit_drop = [('var1(t-%d)' % (j)) for j in range(1,48)]\n",
    "X_taux = df_preprocessed_champs.drop(columns=['var1(t)','var2(t)'] + names_taux_drop)\n",
    "X_debit = df_preprocessed_champs.drop(columns=['var1(t)','var2(t)']+ names_debit_drop)\n",
    "X_taux = X_taux.drop(columns=['Libelle','Date et heure de comptage','date'])\n",
    "X_debit = X_debit.drop(columns=['Libelle','Date et heure de comptage','date'])\n",
    "\n",
    "## Data split \n",
    "X_train_taux, X_test_taux,y_taux_train, y_taux_test = train_test_split(X_taux,y_taux_t, test_size=0.1)\n",
    "X_train_debit, X_test_debit,y_debit_train, y_debit_test = train_test_split(X_debit,y_debit_t, test_size=0.2)\n",
    "\n",
    "y_taux_train_norm = normalize(y_taux_train)\n",
    "\n",
    "y_debit_train_norm = normalize(y_debit_train)\n",
    "\n",
    "## Features type declaration\n",
    "min_max_columns = ['vacances','est_ferie','year_2021','year_2022','hour_0','hour_1','hour_2','hour_3','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23','day_1','day_2','day_3','day_4','day_5','day_6','day_7','day_8','day_9','day_10','day_11','day_12','day_13','day_14','day_15','day_16','day_17','day_18','day_19','day_20','day_21','day_22','day_23','day_24','day_25','day_26','day_27','day_28','day_29','day_30','day_31','wday_Monday','wday_Tuesday','wday_Wednesday','wday_Thursday','wday_Friday','wday_Saturday','wday_Sunday','month_1','month_2','month_3','month_4','month_5','month_6','month_7','month_8','month_9','month_10','month_11','month_12']\n",
    "numerical_col_taux = [feature for feature in list(X_taux) if feature not in min_max_columns]\n",
    "numerical_col_debit = [feature for feature in list(X_debit) if feature not in min_max_columns]\n",
    "\n",
    "## Normalisation : \n",
    "pipeline_taux = create_pipeline(numerical_col_taux,min_max_columns)\n",
    "X_train_taux_preprocessed = pipeline_taux.fit_transform(X_train_taux)\n",
    "X_test_taux_preprocessed = pipeline_taux.transform(X_test_taux)\n",
    "\n",
    "\n",
    "pipeline_debit = create_pipeline(numerical_col_debit,min_max_columns)\n",
    "X_train_debit_preprocessed = pipeline_debit.fit_transform(X_train_debit)\n",
    "X_test_debit_preprocessed = pipeline_debit.transform(X_test_debit)\n",
    "\n",
    "##Modelisation : \n",
    "\n",
    "## Prediction of taux at t : \n",
    "model_taux_t = xgb.XGBRegressor()\n",
    "model_taux_t = model_taux_t.fit(X_train_taux_preprocessed,y_taux_train_norm)\n",
    "\n",
    "## Prediction of débit at t : \n",
    "model_debit_t = xgb.XGBRegressor()\n",
    "model_debit_t = model_debit_t.fit(X_train_debit_preprocessed, y_debit_train_norm)\n",
    "\n",
    "\n",
    "#Testing data \n",
    "y_pred_taux_t = model_taux_t.predict(X_test_taux_preprocessed)\n",
    "y_pred_taux_t = unormalize(y_pred_taux_t, y_taux_train)\n",
    "MSE_taux = mean_squared_error(y_taux_test, y_pred_taux_t)\n",
    "\n",
    "y_pred_debit_t = model_debit_t.predict(X_test_debit_preprocessed)\n",
    "y_pred_debit_t = unormalize(y_pred_debit_t, y_debit_train)\n",
    "MSE_debit = mean_squared_error(y_debit_test,y_pred_debit_t)\n",
    "\n",
    "print(MSE_taux**0.5,y_taux_test.std(),MSE_debit**0.5,y_debit_test.std())\n",
    "\n",
    "#Plot \n",
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(18, 5))\n",
    "n_taux = len(y_taux_test)\n",
    "t_taux = np.arange(n_taux)\n",
    "# plt.figure(\"Taux d'occupation\")\n",
    "ax[0,0].plot(t_taux,y_taux_test, 'r')\n",
    "ax[0,0].plot(t_taux,y_pred_taux_t, 'b')\n",
    "ax[0,0].title.set_text(\"Taux d'occupation\")\n",
    "n_debit= len(y_debit_test)\n",
    "t_debit = np.arange(n_debit)\n",
    "# plt.figure(\"Débit horaire\")\n",
    "ax[0,1].plot(t_debit,y_debit_test, 'r')\n",
    "ax[0,1].plot(t_debit,y_pred_debit_t, 'b')\n",
    "ax[0,1].title.set_text(\"Débit horaire\")\n",
    "\n",
    "t_zoom = np.arange(100)\n",
    "ax[1,0].plot(t_zoom,y_taux_test[:100], 'r')\n",
    "ax[1,0].plot(t_zoom,y_pred_taux_t[:100], 'b')\n",
    "\n",
    "ax[1,1].plot(t_zoom,y_debit_test[:100], 'r')\n",
    "ax[1,1].plot(t_zoom,y_pred_debit_t[:100], 'b')\n",
    "fig.legend(['Real Data','Predicted'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction de K valeurs futures (with outputs noramlized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To reload on Dej because error\n",
    "# params = {'colsample_bytree': 0.3, 'max_depth': 8, 'min_child_weight': 5, 'n_estimators': 43}\n",
    "df_preprocessed_champs_n = series_to_supervised(df_champs, 12, 24)\n",
    "df_preprocessed_champs_n = df_preprocessed_champs_n.drop(columns=[\"Taux d'occupation\",\"Débit horaire\"]) \n",
    "names_taux = ['var1(t)']\n",
    "names_taux += [('var%d(t+%d)' % (1, i)) for i in range(1,24)]\n",
    "y_taux_n =  df_preprocessed_champs_n[names_taux]\n",
    "names_debit = ['var2(t)']\n",
    "names_debit += [('var%d(t+%d)' % (2, i)) for i in range(1,24)]\n",
    "y_debit_n = df_preprocessed_champs_n[names_debit]\n",
    "X_past = df_preprocessed_champs_n.drop(columns=names_debit+names_taux)\n",
    "X_past = X_past.drop(columns=['Libelle','Date et heure de comptage','date'])\n",
    "\n",
    "## Data split \n",
    "X_train_taux_n , X_test_taux_n ,y_taux_train_n , y_taux_test_n = train_test_split(X_past,y_taux_n, test_size=0.2)\n",
    "X_train_debit_n, X_test_debit_n,y_debit_train_n, y_debit_test_n = train_test_split(X_past,y_debit_n, test_size=0.2)\n",
    "\n",
    "y_taux_train_n_norm = normalize_n(y_taux_train_n)\n",
    "y_debit_train_n_norm = normalize_n(y_debit_train_n)\n",
    "\n",
    "## Features type declaration\n",
    "min_max_columns_n = ['vacances','est_ferie','year_2021','year_2022','hour_0','hour_1','hour_2','hour_3','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23','day_1','day_2','day_3','day_4','day_5','day_6','day_7','day_8','day_9','day_10','day_11','day_12','day_13','day_14','day_15','day_16','day_17','day_18','day_19','day_20','day_21','day_22','day_23','day_24','day_25','day_26','day_27','day_28','day_29','day_30','day_31','wday_Monday','wday_Tuesday','wday_Wednesday','wday_Thursday','wday_Friday','wday_Saturday','wday_Sunday','month_1','month_2','month_3','month_4','month_5','month_6','month_7','month_8','month_9','month_10','month_11','month_12']\n",
    "numerical_col_n = [feature for feature in list(X_past) if feature not in min_max_columns_n]\n",
    "\n",
    "#Normalization\n",
    "pipeline_n = create_pipeline(numerical_col_n,min_max_columns_n)\n",
    "X_train_taux_n_preprocessed = pipeline_n.fit_transform(X_train_taux_n)\n",
    "X_test_taux_n_preprocessed = pipeline_n.transform(X_test_taux_n)\n",
    "X_train_debit_n_preprocessed = pipeline_n.fit_transform(X_train_debit_n)\n",
    "X_test_debit_n_preprocessed = pipeline_n.transform(X_test_debit_n)\n",
    "\n",
    "#Models \n",
    "\n",
    "## Prediction of taux at t : \n",
    "print('Launch Model')\n",
    "model_taux_n = xgb.XGBRegressor()\n",
    "model_taux_n = model_taux_n.fit(X_train_taux_n_preprocessed,y_taux_train_n_norm)\n",
    "print('ok') \n",
    "## Prediction of débit at t : \n",
    "model_debit_n = xgb.XGBRegressor()\n",
    "model_debit_n = model_debit_n.fit(X_train_debit_n_preprocessed, y_debit_train_n_norm)\n",
    "print('ok')\n",
    "\n",
    "\n",
    "y_pred_taux_n = model_taux_n.predict(X_test_taux_n_preprocessed)\n",
    "y_pred_taux_n = unormalize_n(y_pred_taux_n, y_taux_train_n)\n",
    "MSE_taux_n = mean_squared_error(y_taux_test_n, y_pred_taux_n)\n",
    "\n",
    "\n",
    "y_pred_debit_n = model_debit_n.predict(X_test_debit_n_preprocessed)\n",
    "y_pred_debit_n = unormalize_n(y_pred_debit_n, y_debit_train_n)\n",
    "MSE_debit_n = mean_squared_error(y_debit_test_n, y_pred_debit_n)\n",
    "\n",
    "print(MSE_taux_n**0.5,MSE_debit_n**0.5)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(18, 5))\n",
    "n_taux_n = len(y_taux_test_n)\n",
    "t_taux_n = np.arange(n_taux_n)\n",
    "# plt.figure(\"Taux d'occupation\")\n",
    "ax[0,0].plot(t_taux_n,y_taux_test_n, 'r')\n",
    "ax[0,0].plot(t_taux_n,y_pred_taux_n, 'b')\n",
    "ax[0,0].title.set_text(\"Taux d'occupation\")\n",
    "\n",
    "\n",
    "n_debit_n= len(y_debit_test_n)\n",
    "t_debit_n = np.arange(n_debit_n)\n",
    "# plt.figure(\"Débit horaire\")\n",
    "ax[0,1].plot(t_debit_n,y_debit_test_n, 'r') \n",
    "ax[0,1].plot(t_debit_n,y_pred_debit_n, 'b')\n",
    "ax[0,1].title.set_text(\"Débit horaire\")\n",
    "\n",
    "t_zoom = np.arange(len(y_taux_test_n.iloc[-1, :]))\n",
    "ax[1,0].plot(t_zoom,y_taux_test_n.iloc[-1, :], 'r')\n",
    "ax[1,0].plot(t_zoom,y_pred_taux_n[-1], 'b')\n",
    "\n",
    "ax[1,1].plot(t_zoom,y_debit_test_n.iloc[0, :], 'r')\n",
    "ax[1,1].plot(t_zoom,y_pred_debit_n[0], 'b')\n",
    "ax[1,1].legend(['real','pred'])\n",
    "# fig.legend(['Real Data','Predicted'])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Tunning : \n",
    "<p>Nous avons utiliser 2 type d'optimisation de feature tunning. Nous avons fait une RFE pour réduire le nombre de paramètres et un HalvingGridSearch pour toruver les paramètres optimaux de xgb.Regressor</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RFE\n",
    "df_preprocessed_champs_n = series_to_supervised(df_champs, 12, 24)\n",
    "df_preprocessed_champs_n = df_preprocessed_champs_n.drop(columns=[\"Taux d'occupation\",\"Débit horaire\"]) \n",
    "names_taux = ['var1(t)']\n",
    "names_taux += [('var%d(t+%d)' % (1, i)) for i in range(1,24)]\n",
    "y_taux_n =  df_preprocessed_champs_n[names_taux]\n",
    "names_debit = ['var2(t)']\n",
    "names_debit += [('var%d(t+%d)' % (2, i)) for i in range(1,24)]\n",
    "y_debit_n = df_preprocessed_champs_n[names_debit]\n",
    "X_past = df_preprocessed_champs_n.drop(columns=names_debit+names_taux)\n",
    "X_past = X_past.drop(columns=['Libelle','Date et heure de comptage','date'])\n",
    "\n",
    "X_train_taux_n , X_test_taux_n ,y_taux_train_n , y_taux_test_n = train_test_split(X_past,y_taux_n, test_size=0.2, random_state=1)\n",
    "X_train_debit_n, X_test_debit_n,y_debit_train_n, y_debit_test_n = train_test_split(X_past,y_debit_n, test_size=0.2, random_state=1)\n",
    "\n",
    "y_taux_train_n_norm = normalize_n(y_taux_train_n)\n",
    "y_debit_train_n_norm = normalize_n(y_debit_train_n)\n",
    "\n",
    "## Features type declaration\n",
    "min_max_columns_n = ['vacances','est_ferie','year_2021','year_2022','hour_0','hour_1','hour_2','hour_3','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23','day_1','day_2','day_3','day_4','day_5','day_6','day_7','day_8','day_9','day_10','day_11','day_12','day_13','day_14','day_15','day_16','day_17','day_18','day_19','day_20','day_21','day_22','day_23','day_24','day_25','day_26','day_27','day_28','day_29','day_30','day_31','wday_Monday','wday_Tuesday','wday_Wednesday','wday_Thursday','wday_Friday','wday_Saturday','wday_Sunday','month_1','month_2','month_3','month_4','month_5','month_6','month_7','month_8','month_9','month_10','month_11','month_12']\n",
    "numerical_col_n = [feature for feature in list(X_past) if feature not in min_max_columns_n]\n",
    "\n",
    "#Normalization\n",
    "pipeline_n = create_pipeline(numerical_col_n,min_max_columns_n)\n",
    "X_train_taux_n_preprocessed = pipeline_n.fit_transform(X_train_taux_n)\n",
    "X_test_taux_n_preprocessed = pipeline_n.transform(X_test_taux_n)\n",
    "X_train_debit_n_preprocessed = pipeline_n.fit_transform(X_train_debit_n)\n",
    "X_test_debit_n_preprocessed = pipeline_n.transform(X_test_debit_n)\n",
    "\n",
    "model_taux_n = xgb.XGBRegressor()\n",
    "RFE_taux = RFE(estimator=model_taux_n, n_features_to_select=69, step=1)\n",
    "RFE_taux.fit(X_past,y_taux_n)\n",
    "## Prediction of débit at t : \n",
    "model_debit_n = xgb.XGBRegressor(X_past,y_debit_n)\n",
    "RFE_debit = RFE(estimator=model_debit_n, n_features_to_select=69, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid Search \n",
    "gbm_params_grid = {\n",
    "    'max_depth':[5,6,7,8,9],\n",
    "    'min_child_weight': [4,5,6,7,8],\n",
    "    'n_estimators': [i for i in range(30,51)], \n",
    "    'colsample_bytree':[0.1,0.2,0.3]\n",
    "}\n",
    "estim = xgb.XGBRegressor()\n",
    "search = HalvingGridSearchCV(estim, gbm_params_grid,cv=3,scoring='neg_mean_squared_error', n_jobs=-1).fit(X_past,y_debit_n)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (pipeline)",
   "language": "python",
   "name": "kedro_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a816deead78f08b2a4ea31f713b7c8fe6cef1f1d74375017fc975e555f45423b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
